---
type: lecture
date: 2025-03-03
title: (dl-15) Self-Attention and Transformers

# optional
# please use /static_files/notes directory to store notes
# thumbnail: /static_files/path/to/image.jpg 

# optional
tldr: "Attention is all we need!"
  
# optional
# set it to true if you dont want this lecture to appear in the updates section
hide_from_announcments: false

# optional
links: 
    #- url: /static_files/sgd_update_rules.gif
    #  name: sgd_update_rules.gif
    #- url: https://colab.research.google.com/drive/1OHmvKM7wINUS9kBDQExY_oDKK4AX-wgN?usp=sharing
    #  name: Sample-sequential-task
    - url: /static_files/presentations/dl-15-I.pdf
      name: slides-I
    - url: /static_files/presentations/dl-15-II.pdf
      name: slides-II
    - url: /static_files/additional/transformer_decoding_1.gif
      name: GIF-1 (decoding)
    - url: /static_files/additional/transformer_decoding_2.gif
      name: GIF-2  (decoding)
---
**Suggested Readings:**
- [Chapter-12 from the Bishop's book](https://www.bishopbook.com/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention is all we need!](https://arxiv.org/pdf/1706.03762.pdf)
- [Computational complexity discussion](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model)
- [My slides on self-attention layer for CNNs](https://docs.google.com/presentation/d/15ifYMrvQtDrp20H8JWbP94Tifh8OWHlxXt1hiGfpyh0/edit?usp=sharing)
