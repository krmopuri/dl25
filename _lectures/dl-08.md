---
type: lecture
date: 2025-01-27
title: (dl-08) Training DNNs - I

# optional
# please use /static_files/notes directory to store notes
# thumbnail: /static_files/path/to/image.jpg

# optional
tldr: "Issues with Gradient Descent, specifically with the learning rate "
  
# optional
# set it to true if you dont want this lecture to appear in the updates section
hide_from_announcments: false

# optional
links: 
    #- url: /static_files/presentations/lec.zip
    #  name: notes
    - url: https://colab.research.google.com/drive/1L5yEFMevk9EYiWa1V4p7-uTDA7cQEmCn?usp=sharing
      name: codes - Optimizing 1D Convex function
    - url: https://colab.research.google.com/drive/1eMBFrC9ms-qQVVUJOGJL1Pm28ehKKI4N?usp=sharing
      name: codes - Optimizing 2D Quadratic function
    - url: https://drive.google.com/file/d/1jMxvrO_zOEtNL3wLTayV5cKkBiw9rnq8/view?usp=sharing
      name: slides
    #- url: /static_files/presentations/lec.zip
    #  name: other
---

**Suggested Readings:**

- [Chapter-7 from Bishop's Book](https://www.bishopbook.com/)
- [Ruder's blog on variants of gradient-based updates](https://www.ruder.io/optimizing-gradient-descent/)
- [Chapter on Optimization from Goodfellow et al. book](https://www.deeplearningbook.org/contents/optimization.html)
- [Chapter 5 from Michael Nielson's NNDL book](http://neuralnetworksanddeeplearning.com/chap5.html)

